
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Resampling Methods &#8212; Statistics and Machine Learning in Python 0.4 beta documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ensemble learning: bagging, boosting and stacking" href="ensemble_learning.html" />
    <link rel="prev" title="Non linear learning algorithms" href="non_linear_prediction.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="resampling-methods">
<h1>Resampling Methods<a class="headerlink" href="#resampling-methods" title="Permalink to this headline">¶</a></h1>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
<div class="section" id="train-validation-and-test-sets">
<h2>Train, Validation and Test Sets<a class="headerlink" href="#train-validation-and-test-sets" title="Permalink to this headline">¶</a></h2>
<p>Machine learning algorithms overfit taining data. Predictive
performances <strong>MUST</strong> be evaluated on independant hold-out dataset.</p>
<div class="figure align-default" id="id2">
<img alt="Train, Validation and Test Sets." src="../_images/train_validation_test_sets.png" />
<p class="caption"><span class="caption-text">Train, Validation and Test Sets.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<ol class="arabic simple">
<li><p><strong>Training dataset</strong>: Dataset used to fit the model (set the model
parameters like weights). The <em>training error</em> can be easily
calculated by applying the statistical learning method to the
observations used in its training. But because of overfitting, the
<strong>training error rate can dramatically underestimate the error</strong> that
would be obtained on new samples.</p></li>
<li><p><strong>Validation dataset</strong>: Dataset used to provide an unbiased
evaluation of a model fit on the training dataset while <strong>tuning
model hyperparameters</strong>. The validation error is the average error
that results from a learning method to predict the response on a new
(validation) samples that is, on samples that were not used in
training the method.</p></li>
<li><p><strong>Test Dataset</strong>: Dataset used to provide an unbiased evaluation of a
final model fit on the training dataset. It is only used once a model
is completely trained(using the train and validation sets).</p></li>
</ol>
<p>What is the Difference Between Test and Validation Datasets? by <a class="reference external" href="https://machinelearningmastery.com/difference-test-validation-datasets/">Jason
Brownlee</a></p>
<p>Thus the original dataset is generally split in a training, validation
and a test data sets. Large training+validation set (80%) small test set
(20%) might provide a poor estimation of the predictive performances
(same argument stands for train vs validation samples). On the contrary,
large test set and small training set might produce a poorly estimated
learner. This is why, on situation where we cannot afford such split, it
recommended to use cross-validation scheme to estimate the predictive
power of a learning algorithm.</p>
</div>
<div class="section" id="cross-validation-cv">
<h2>Cross-Validation (CV)<a class="headerlink" href="#cross-validation-cv" title="Permalink to this headline">¶</a></h2>
<p>Cross-Validation scheme randomly divides the set of observations into
<span class="math notranslate nohighlight">\(K\)</span> groups, or <strong>folds</strong>, of approximately equal size. The first
fold is treated as a validation set, and the method <span class="math notranslate nohighlight">\(f()\)</span> is
fitted on the remaining union of <span class="math notranslate nohighlight">\(K - 1\)</span> folds:
(<span class="math notranslate nohighlight">\(f(\boldsymbol{X}_{-K}, \boldsymbol{y}_{-K})\)</span>).</p>
<p>The measure of performance (the score function <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>),
either a error measure or an correct prediction measure is an average of
a loss error or correct prediction measure, noted <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>,
between a true target value and the predicted target value. The score
function is evaluated of the on the observations in the held-out fold.
For each sample <span class="math notranslate nohighlight">\(i\)</span> we consider the model estimated
<span class="math notranslate nohighlight">\(f(\boldsymbol{X}_{-k(i)}, \boldsymbol{y}_{-k(i)}\)</span> on the data set
without the group <span class="math notranslate nohighlight">\(k\)</span> that contains <span class="math notranslate nohighlight">\(i\)</span> noted <span class="math notranslate nohighlight">\(-k(i)\)</span>.
This procedure is repeated <span class="math notranslate nohighlight">\(K\)</span> times; each time, a different group
of observations is treated as a test set. Then we compare the predicted
value (<span class="math notranslate nohighlight">\(f_{-k(i)}(\boldsymbol{x}_i) = \hat{y_i})\)</span> with true value
<span class="math notranslate nohighlight">\(y_i\)</span> using a Error or Loss function
<span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>.</p>
<p>For 10-fold we can either average over 10 values (Macro measure) or
concatenate the 10 experiments and compute the micro measures.</p>
<p>Two strategies <a class="reference external" href="https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation">micro vs macro
estimates</a>:</p>
<p><strong>Micro measure: average(individual scores)</strong>: compute a score
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span> for each sample and average over all samples. It is
simillar to <strong>average score(concatenation)</strong>: an averaged score computed
over all concatenated samples.</p>
<div class="math notranslate nohighlight">
\[\mathcal{S}(f) = \frac{1}{N} \sum_i^N \mathcal{L}\left(y_i, f(\boldsymbol{x}_{-k(i)}, \boldsymbol{y}_{-k(i)}) \right).\]</div>
<p><strong>Macro measure mean(CV scores)</strong> (the most commonly used method):
compute a score <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> on each each fold <span class="math notranslate nohighlight">\(k\)</span> and
average accross folds:</p>
<p>These two measures (an average of average vs. a global average) are
generaly similar. They may differ slightly is folds are of different
sizes.</p>
<p>This validation scheme is known as the <strong>K-Fold CV</strong>. Typical choices of
<span class="math notranslate nohighlight">\(K\)</span> are 5 or 10, [Kohavi 1995]. The extreme case where
<span class="math notranslate nohighlight">\(K = N\)</span> is known as <strong>leave-one-out cross-validation, LOO-CV</strong>.</p>
<div class="section" id="cv-for-regression">
<h3>CV for regression<a class="headerlink" href="#cv-for-regression" title="Permalink to this headline">¶</a></h3>
<p>Usually the error function <span class="math notranslate nohighlight">\(\mathcal{L}()\)</span> is the r-squared score.
However other function could be used.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;once&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">r2_train</span><span class="p">,</span> <span class="n">r2_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">r2_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="n">r2_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Train</span> <span class="n">r2</span><span class="p">:</span><span class="mf">0.99</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.73</span>
</pre></div>
</div>
<p>Scikit-learn provides user-friendly function to perform CV:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># provide a cv</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.73</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.73</span>
</pre></div>
</div>
</div>
<div class="section" id="cv-for-classification">
<h3>CV for classification<a class="headerlink" href="#cv-for-classification" title="Permalink to this headline">¶</a></h3>
<p>With classification problems it is essential to sample folds where each
set contains approximately the same percentage of samples of each target
class as the complete set. This is called <strong>stratification</strong>. In this
case, we will use <code class="docutils literal notranslate"><span class="pre">StratifiedKFold</span></code> with is a variation of k-fold
which returns stratified folds.</p>
<p>Usually the error function <span class="math notranslate nohighlight">\(L()\)</span> are, at least, the sensitivity
and the specificity. However other function could be used.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Lists to store scores by folds (for macro measure only)</span>
<span class="n">recalls_train</span><span class="p">,</span> <span class="n">recalls_test</span><span class="p">,</span> <span class="n">acc_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

<span class="c1"># Or vector of test predictions (for both macro and micro measures, not for training samples)</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">recalls_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
    <span class="n">recalls_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
    <span class="n">acc_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])))</span>

    <span class="c1"># Store test predictions (for micro measures)</span>
    <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Macro measures ==&quot;</span><span class="p">)</span>
<span class="c1"># Use lists of scores</span>
<span class="n">recalls_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">recalls_train</span><span class="p">)</span>
<span class="n">recalls_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">recalls_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train SPC:</span><span class="si">%.2f</span><span class="s2">; SEN:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">recalls_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  SPC:</span><span class="si">%.2f</span><span class="s2">; SEN:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">recalls_test</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">, ballanced ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">acc_test</span><span class="p">),</span> <span class="n">recalls_test</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()),</span> <span class="s2">&quot;Folds:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>

<span class="c1"># Or use vector to test predictions</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">acc_test</span><span class="p">),</span> <span class="s2">&quot;Folds:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Micro measures ==&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test SPC:</span><span class="si">%.2f</span><span class="s2">; SEN:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> \
      <span class="nb">tuple</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span> <span class="n">Macro</span> <span class="n">measures</span> <span class="o">==</span>
<span class="n">Train</span> <span class="n">SPC</span><span class="p">:</span><span class="mf">1.00</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span><span class="mf">1.00</span>
<span class="n">Test</span>  <span class="n">SPC</span><span class="p">:</span><span class="mf">0.78</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span><span class="mf">0.82</span>
<span class="n">Test</span>  <span class="n">ACC</span><span class="p">:</span><span class="mf">0.80</span><span class="p">,</span> <span class="n">ballanced</span> <span class="n">ACC</span><span class="p">:</span><span class="mf">0.80</span> <span class="n">Folds</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">Test</span>  <span class="n">ACC</span><span class="p">:</span><span class="mf">0.80</span> <span class="n">Folds</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="o">==</span> <span class="n">Micro</span> <span class="n">measures</span> <span class="o">==</span>
<span class="n">Test</span> <span class="n">SPC</span><span class="p">:</span><span class="mf">0.78</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span><span class="mf">0.82</span>
<span class="n">Test</span>  <span class="n">ACC</span><span class="p">:</span><span class="mf">0.80</span>
</pre></div>
</div>
<p>Scikit-learn provides user-friendly function to perform CV:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># provide CV and score</span>
<span class="k">def</span> <span class="nf">balanced_acc</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Balanced acuracy scorer</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">balanced_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span>  <span class="n">ACC</span><span class="p">:</span><span class="mf">0.80</span>
</pre></div>
</div>
<p>Note that with Scikit-learn user-friendly function we average the
scores’ average obtained on individual folds which may provide slightly
different results that the overall average presented earlier.</p>
</div>
</div>
<div class="section" id="parallel-computation-with-joblib">
<h2>Parallel computation with joblib<a class="headerlink" href="#parallel-computation-with-joblib" title="Permalink to this headline">¶</a></h2>
<p>Dataset</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="use-cross-validate-function">
<h3>Use <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function<a class="headerlink" href="#use-cross-validate-function" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_score&#39;</span><span class="p">]),</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_score&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.8</span> <span class="p">[</span><span class="mf">0.5</span> <span class="mf">0.5</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span> <span class="p">]</span>
</pre></div>
</div>
<p>### Sequential computation</p>
<p>If we want have full control of the operations performed within each
fold (retrieve the models parameters, etc.). We would like to
parallelize the folowing sequetial code:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">y_test_pred_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="c1"># Store predictions in the original order</span>
<span class="n">coefs_seq</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test_pred_seq</span><span class="p">[</span><span class="n">test</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">coefs_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">test_accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_test_pred_seq</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">),</span> <span class="n">test_accs</span><span class="p">)</span>
<span class="n">coefs_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_seq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefs_cv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std Err of the coef&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.8</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">[[[</span><span class="o">-</span><span class="mf">0.87692513</span>  <span class="mf">0.6260013</span>   <span class="mf">1.18714373</span> <span class="o">-</span><span class="mf">0.30685978</span> <span class="o">-</span><span class="mf">0.38037393</span><span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.7464993</span>   <span class="mf">0.62138165</span>  <span class="mf">1.10144804</span>  <span class="mf">0.19800115</span> <span class="o">-</span><span class="mf">0.40112109</span><span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.96020317</span>  <span class="mf">0.51135134</span>  <span class="mf">1.1210943</span>   <span class="mf">0.08039112</span> <span class="o">-</span><span class="mf">0.2643663</span> <span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.85755505</span>  <span class="mf">0.52010552</span>  <span class="mf">1.06637346</span> <span class="o">-</span><span class="mf">0.10994258</span> <span class="o">-</span><span class="mf">0.29152132</span><span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.89914467</span>  <span class="mf">0.51481483</span>  <span class="mf">1.08675378</span> <span class="o">-</span><span class="mf">0.24767837</span> <span class="o">-</span><span class="mf">0.27899525</span><span class="p">]]]</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.86806546</span>  <span class="mf">0.55873093</span>  <span class="mf">1.11256266</span> <span class="o">-</span><span class="mf">0.07721769</span> <span class="o">-</span><span class="mf">0.32327558</span><span class="p">]]</span>
<span class="n">Std</span> <span class="n">Err</span> <span class="n">of</span> <span class="n">the</span> <span class="n">coef</span>
<span class="p">[[</span><span class="mf">0.03125544</span> <span class="mf">0.02376198</span> <span class="mf">0.01850211</span> <span class="mf">0.08566194</span> <span class="mf">0.02510739</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h3>Parallel computation with joblib<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.externals.joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">is_classifier</span><span class="p">,</span> <span class="n">clone</span>

<span class="k">def</span> <span class="nf">_split_fit_predict</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">estimator</span><span class="o">.</span><span class="n">coef_</span><span class="p">]</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>

<span class="n">parallel</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">cv_ret</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
    <span class="n">delayed</span><span class="p">(</span><span class="n">_split_fit_predict</span><span class="p">)(</span>
        <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="n">y_test_pred_cv</span><span class="p">,</span> <span class="n">coefs_cv</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">cv_ret</span><span class="p">)</span>

<span class="c1"># Retrieve predictions in the original order</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test_pred_cv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">test_accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">),</span> <span class="n">test_accs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.8</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
</pre></div>
</div>
<p>Test same predictions and same coeficients</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">y_test_pred_seq</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_cv</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_seq</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="cv-for-model-selection-setting-the-hyper-parameters">
<h2>CV for model selection: setting the hyper parameters<a class="headerlink" href="#cv-for-model-selection-setting-the-hyper-parameters" title="Permalink to this headline">¶</a></h2>
<p>It is important to note CV may be used for two separate goals:</p>
<ol class="arabic simple">
<li><p><strong>Model assessment</strong>: having chosen a final model, estimating its
prediction error (generalization error) on new data.</p></li>
<li><p><strong>Model selection</strong>: estimating the performance of different models
in order to choose the best one. One special case of model selection
is the selection model’s hyper parameters. Indeed remember that most
of learning algorithm have a hyper parameters (typically the
regularization parameter) that has to be set.</p></li>
</ol>
<p>Generally we must address the two problems simultaneously. The usual
approach for both problems is to randomly divide the dataset into three
parts: a training set, a validation set, and a test set.</p>
<ul class="simple">
<li><p>The <strong>training set</strong> (train) is used to fit the models;</p></li>
<li><p>the <strong>validation set</strong> (val) is used to estimate prediction error for
model selection or to determine the hyper parameters over a grid of
possible values.</p></li>
<li><p>the <strong>test set</strong> (test) is used for assessment of the generalization
error of the final chosen model.</p></li>
</ul>
<div class="section" id="grid-search-procedure">
<h3>Grid search procedure<a class="headerlink" href="#grid-search-procedure" title="Permalink to this headline">¶</a></h3>
<p>Model selection of the best hyper parameters over a grid of possible
values</p>
<p>For each possible values of hyper parameters <span class="math notranslate nohighlight">\(\alpha_k\)</span>:</p>
<ol class="arabic">
<li><p>Fit the learner on training set:
<span class="math notranslate nohighlight">\(f(X_{train}, y_{train}, \alpha_k)\)</span></p></li>
<li><p>Evaluate the model on the validation set and keep the parameter(s)
that minimises the error measure</p>
<p><span class="math notranslate nohighlight">\(\alpha_* = \arg \min L(f(X_{train}), y_{val}, \alpha_k)\)</span></p>
</li>
<li><p>Refit the learner on all training + validation data using the best
hyper parameters:
<span class="math notranslate nohighlight">\(f^* \equiv f(X_{train \cup val}, y_{train \cup val}, \alpha_*)\)</span></p></li>
<li><p>** Model assessment ** of <span class="math notranslate nohighlight">\(f^*\)</span> on the test set:
<span class="math notranslate nohighlight">\(L(f^*(X_{test}), y_{test})\)</span></p></li>
</ol>
</div>
<div class="section" id="nested-cv-for-model-selection-and-assessment">
<h3>Nested CV for model selection and assessment<a class="headerlink" href="#nested-cv-for-model-selection-and-assessment" title="Permalink to this headline">¶</a></h3>
<p>Most of time, we cannot afford such three-way split. Thus, again we will
use CV, but in this case we need two nested CVs.</p>
<p>One <strong>outer CV loop, for model assessment</strong>. This CV performs <span class="math notranslate nohighlight">\(K\)</span>
splits of the dataset into training plus validation
(<span class="math notranslate nohighlight">\(X_{-K}, y_{-K}\)</span>) set and a test set <span class="math notranslate nohighlight">\(X_{K}, y_{K}\)</span></p>
<p>One <strong>inner CV loop, for model selection</strong>. For each run of the outer
loop, the inner loop loop performs <span class="math notranslate nohighlight">\(L\)</span> splits of dataset
(<span class="math notranslate nohighlight">\(X_{-K}, y_{-K}\)</span>) into training set:
(<span class="math notranslate nohighlight">\(X_{-K,-L}, y_{-K,-L}\)</span>) and a validation set:
(<span class="math notranslate nohighlight">\(X_{-K,L}, y_{-K,L}\)</span>).</p>
</div>
<div class="section" id="implementation-with-scikit-learn">
<h3>Implementation with scikit-learn<a class="headerlink" href="#implementation-with-scikit-learn" title="Permalink to this headline">¶</a></h3>
<p>Note that the inner CV loop combined with the learner form a new learner
with an automatic model (parameter) selection procedure. This new
learner can be easily constructed using Scikit-learn. The learned is
wrapped inside a <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> class.</p>
<p>Then the new learned can be plugged into the classical outer CV loop.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="c1"># Dataset</span>
<span class="n">noise_sd</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise_sd</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Use this to tune the noise parameter such that snr &lt; 5</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SNR:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coef</span><span class="p">))</span> <span class="o">/</span> <span class="n">noise_sd</span><span class="p">)</span>

<span class="c1"># param grid over alpha &amp; l1_ratio</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">10.</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">]}</span>

<span class="c1"># Warp</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">ElasticNet</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SNR</span><span class="p">:</span> <span class="mf">2.6358469446381614</span>
</pre></div>
</div>
</div>
<div class="section" id="regression-models-with-built-in-cross-validation">
<h3>Regression models with built-in cross-validation<a class="headerlink" href="#regression-models-with-built-in-cross-validation" title="Permalink to this headline">¶</a></h3>
<p>Sklearn will automatically select a grid of parameters, most of time use
the defaults values.</p>
<p><code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> is the number of CPUs to use during the cross validation. If
-1, use all the CPUs.</p>
<ol class="arabic simple">
<li><p>Biased usage: fit on all data, ommit outer CV loop</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Train</span> <span class="n">r2</span><span class="p">:</span><span class="mf">0.96</span>
<span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>User made outer CV, useful to extract specific information</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">r2_train</span><span class="p">,</span> <span class="n">r2_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">r2_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
    <span class="n">r2_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>

    <span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Selected alphas:&quot;</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Train</span> <span class="n">r2</span><span class="p">:</span><span class="mf">1.00</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.55</span>
<span class="n">Selected</span> <span class="n">alphas</span><span class="p">:</span> <span class="p">[{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">}]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>User-friendly sklearn for outer CV</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.55</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Ridge (L2 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Let sklearn select a list of alphas with default LOO-CV</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Lasso (L1 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Let sklearn select a list of alphas with default 3CV</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== ElasticNet (L1 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">l1_ratio</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">],</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Let sklearn select a list of alphas with default 3CV</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span> <span class="n">Ridge</span> <span class="p">(</span><span class="n">L2</span> <span class="n">penalty</span><span class="p">)</span> <span class="o">==</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.16</span>
<span class="o">==</span> <span class="n">Lasso</span> <span class="p">(</span><span class="n">L1</span> <span class="n">penalty</span><span class="p">)</span> <span class="o">==</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.74</span>
<span class="o">==</span> <span class="n">ElasticNet</span> <span class="p">(</span><span class="n">L1</span> <span class="n">penalty</span><span class="p">)</span> <span class="o">==</span>
<span class="n">Test</span>  <span class="n">r2</span><span class="p">:</span><span class="mf">0.58</span>
</pre></div>
</div>
</div>
<div class="section" id="classification-models-with-built-in-cross-validation">
<h3>Classification models with built-in cross-validation<a class="headerlink" href="#classification-models-with-built-in-cross-validation" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># provide CV and score</span>
<span class="k">def</span> <span class="nf">balanced_acc</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Balanced accuracy scorer</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Logistic Ridge (L2 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">balanced_acc</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Let sklearn select a list of alphas with default LOO-CV</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==</span> <span class="n">Logistic</span> <span class="n">Ridge</span> <span class="p">(</span><span class="n">L2</span> <span class="n">penalty</span><span class="p">)</span> <span class="o">==</span>
<span class="n">Test</span>  <span class="n">ACC</span><span class="p">:</span><span class="mf">0.77</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="random-permutations">
<h2>Random Permutations<a class="headerlink" href="#random-permutations" title="Permalink to this headline">¶</a></h2>
<p>A permutation test is a type of non-parametric randomization test in
which the null distribution of a test statistic is estimated by randomly
permuting the observations.</p>
<p>Permutation tests are highly attractive because they make no assumptions
other than that the observations are independent and identically
distributed under the null hypothesis.</p>
<ol class="arabic simple">
<li><p>Compute a observed statistic <span class="math notranslate nohighlight">\(t_{obs}\)</span> on the data.</p></li>
<li><p>Use randomization to compute the distribution of <span class="math notranslate nohighlight">\(t\)</span> under the
null hypothesis: Perform <span class="math notranslate nohighlight">\(N\)</span> random permutation of the data.
For each sample of permuted data, <span class="math notranslate nohighlight">\(i\)</span> the data compute the
statistic <span class="math notranslate nohighlight">\(t_i\)</span>. This procedure provides the distribution of
<span class="math notranslate nohighlight">\(t\)</span> under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>:
<span class="math notranslate nohighlight">\(P(t \vert H_0)\)</span></p></li>
<li><p>Compute the p-value =
<span class="math notranslate nohighlight">\(P(t&gt;t_{obs} | H_0) \left\vert\{t_i &gt; t_{obs}\}\right\vert\)</span>,
where <span class="math notranslate nohighlight">\(t_i\)</span>’s include <span class="math notranslate nohighlight">\(t_{obs}\)</span>.</p></li>
</ol>
<div class="section" id="example-with-a-correlation">
<h3>Example with a correlation<a class="headerlink" href="#example-with-a-correlation" title="Permalink to this headline">¶</a></h3>
<p>The statistic is the correlation.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1">#%matplotlib qt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># snr = 1/2</span>

<span class="c1"># Permutation: simulate the null hypothesis</span>
<span class="n">nperm</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">perms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nperm</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nperm</span><span class="p">):</span>
    <span class="n">perms</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Plot</span>
<span class="c1"># Re-weight to obtain distribution</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="n">perms</span><span class="p">[</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">perms</span><span class="p">],</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span>
         <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;t&gt;t obs (p-value)&quot;</span><span class="p">,</span> <span class="s2">&quot;t&lt;t obs&quot;</span><span class="p">],</span>
         <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">weights</span><span class="p">[</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">weights</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Statistic distribution under null hypothesis&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;observed statistic&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>

<span class="c1"># One-tailed empirical p-value</span>
<span class="n">pval_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Compare with Pearson&#39;s correlation test</span>
<span class="n">_</span><span class="p">,</span> <span class="n">pval_test</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Permutation two tailed p-value=</span><span class="si">%.5f</span><span class="s2">. Pearson test p-value=</span><span class="si">%.5f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pval_perm</span><span class="p">,</span> <span class="n">pval_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Permutation</span> <span class="n">two</span> <span class="n">tailed</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="o">=</span><span class="mf">0.06959</span><span class="o">.</span> <span class="n">Pearson</span> <span class="n">test</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="o">=</span><span class="mf">0.07355</span>
</pre></div>
</div>
<img alt="../_images/resampling_36_1.png" src="../_images/resampling_36_1.png" />
</div>
<div class="section" id="exercise">
<h3>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h3>
<p>Given the logistic regression presented above and its validation given a
5 folds CV.</p>
<ol class="arabic simple">
<li><p>Compute the p-value associated with the prediction accuracy using a
permutation test.</p></li>
<li><p>Compute the p-value associated with the prediction accuracy using a
parametric test.</p></li>
</ol>
</div>
</div>
<div class="section" id="bootstrapping">
<h2>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Permalink to this headline">¶</a></h2>
<p>Bootstrapping is a random sampling with replacement strategy which
provides an non-parametric method to assess the variability of
performances scores such standard errors or <a class="reference external" href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html#the-bootstrap-method-and-empirical-confidence-intervals">confidence
intervals.</a></p>
<p>A great advantage of bootstrap is its simplicity. It is a
straightforward way to derive estimates of standard errors and
confidence intervals for complex estimators of complex parameters of the
distribution, such as percentile points, proportions, odds ratio, and
correlation coefficients.</p>
<ol class="arabic simple">
<li><p>Perform <span class="math notranslate nohighlight">\(B\)</span> sampling, with replacement, of the dataset.</p></li>
<li><p>For each sample <span class="math notranslate nohighlight">\(i\)</span> fit the model and compute the scores.</p></li>
<li><p>Assess standard errors and confidence intervals of scores using the
scores obtained on the <span class="math notranslate nohighlight">\(B\)</span> resampled dataset.</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Regression dataset</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_features_info</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span><span class="p">[:</span><span class="n">n_features_info</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Xbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Xbeta</span> <span class="o">+</span> <span class="n">eps</span>

<span class="c1"># Fit model on all data (!! risk of overfit)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients on all data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Bootstrap loop</span>
<span class="n">nboot</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># !! Should be at least 1000</span>
<span class="n">scores_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r2&quot;</span><span class="p">]</span>
<span class="n">scores_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nboot</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores_names</span><span class="p">)))</span>
<span class="n">coefs_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nboot</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">orig_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">boot_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nboot</span><span class="p">):</span>
    <span class="n">boot_tr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">orig_all</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">orig_all</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">boot_te</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">orig_all</span><span class="p">,</span> <span class="n">boot_tr</span><span class="p">,</span> <span class="n">assume_unique</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">boot_tr</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">boot_tr</span><span class="p">]</span>
    <span class="n">Xte</span><span class="p">,</span> <span class="n">yte</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">boot_te</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">boot_te</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">scores_boot</span><span class="p">[</span><span class="n">boot_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">coefs_boot</span><span class="p">[</span><span class="n">boot_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

<span class="c1"># Compute Mean, SE, CI</span>
<span class="n">scores_boot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_boot</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">scores_names</span><span class="p">)</span>
<span class="n">scores_stat</span> <span class="o">=</span> <span class="n">scores_boot</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">975</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">025</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r-squared: Mean=</span><span class="si">%.2f</span><span class="s2">, SE=</span><span class="si">%.2f</span><span class="s2">, CI=(</span><span class="si">%.2f</span><span class="s2"> </span><span class="si">%.2f</span><span class="s2">)&quot;</span> <span class="o">%</span>\
      <span class="nb">tuple</span><span class="p">(</span><span class="n">scores_stat</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">,</span> <span class="s2">&quot;5%&quot;</span><span class="p">,</span> <span class="s2">&quot;95%&quot;</span><span class="p">],</span> <span class="s2">&quot;r2&quot;</span><span class="p">]))</span>

<span class="n">coefs_boot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs_boot</span><span class="p">)</span>
<span class="n">coefs_stat</span> <span class="o">=</span> <span class="n">coefs_boot</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">975</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">025</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients distribution&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefs_stat</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficients</span> <span class="n">on</span> <span class="nb">all</span> <span class="n">data</span><span class="p">:</span>
<span class="p">[</span> <span class="mf">1.0257263</span>   <span class="mf">1.11323</span>    <span class="o">-</span><span class="mf">0.0499828</span>  <span class="o">-</span><span class="mf">0.09263008</span>  <span class="mf">0.15267576</span><span class="p">]</span>
<span class="n">r</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span> <span class="n">Mean</span><span class="o">=</span><span class="mf">0.61</span><span class="p">,</span> <span class="n">SE</span><span class="o">=</span><span class="mf">0.10</span><span class="p">,</span> <span class="n">CI</span><span class="o">=</span><span class="p">(</span><span class="n">nan</span> <span class="n">nan</span><span class="p">)</span>
<span class="n">Coefficients</span> <span class="n">distribution</span>
                <span class="mi">0</span>           <span class="mi">1</span>           <span class="mi">2</span>           <span class="mi">3</span>           <span class="mi">4</span>
<span class="n">count</span>  <span class="mf">100.000000</span>  <span class="mf">100.000000</span>  <span class="mf">100.000000</span>  <span class="mf">100.000000</span>  <span class="mf">100.000000</span>
<span class="n">mean</span>     <span class="mf">1.012534</span>    <span class="mf">1.132775</span>   <span class="o">-</span><span class="mf">0.056369</span>   <span class="o">-</span><span class="mf">0.100046</span>    <span class="mf">0.164236</span>
<span class="n">std</span>      <span class="mf">0.094269</span>    <span class="mf">0.104934</span>    <span class="mf">0.111308</span>    <span class="mf">0.095098</span>    <span class="mf">0.095656</span>
<span class="nb">min</span>      <span class="mf">0.759189</span>    <span class="mf">0.836394</span>   <span class="o">-</span><span class="mf">0.290386</span>   <span class="o">-</span><span class="mf">0.318755</span>   <span class="o">-</span><span class="mf">0.092498</span>
<span class="mf">2.5</span><span class="o">%</span>     <span class="mf">0.814260</span>    <span class="mf">0.948158</span>   <span class="o">-</span><span class="mf">0.228483</span>   <span class="o">-</span><span class="mf">0.268790</span>   <span class="o">-</span><span class="mf">0.044067</span>
<span class="mi">50</span><span class="o">%</span>      <span class="mf">1.013097</span>    <span class="mf">1.125304</span>   <span class="o">-</span><span class="mf">0.057039</span>   <span class="o">-</span><span class="mf">0.099281</span>    <span class="mf">0.164194</span>
<span class="mf">97.5</span><span class="o">%</span>    <span class="mf">1.170183</span>    <span class="mf">1.320637</span>    <span class="mf">0.158680</span>    <span class="mf">0.085064</span>    <span class="mf">0.331809</span>
<span class="nb">max</span>      <span class="mf">1.237874</span>    <span class="mf">1.340585</span>    <span class="mf">0.291111</span>    <span class="mf">0.151059</span>    <span class="mf">0.450812</span>
</pre></div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Statistics and Machine Learning in Python</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/python_ecosystem.html">Python ecosystem for data-science</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html#data-analysis-methodology">Data analysis methodology</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html">Import libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#basic-operations">Basic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#execution-control-statements">Execution control statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#list-comprehensions-iterators-etc">List comprehensions, iterators, etc.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#functions">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#regular-expression">Regular expression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#system-programming">System programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#scripts-and-argument-parsing">Scripts and argument parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#networking">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#modules-and-packages">Modules and packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#object-oriented-programming-oop">Object Oriented Programming (OOP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#style-guide-for-python-programming">Style guide for Python programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#documenting">Documenting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#exercises">Exercises</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_numpy.html">Numpy: arrays and matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_pandas.html">Pandas: data manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scientific_python/scipy_matplotlib.html">data visualization: Matplotlib &amp; Seaborn</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_univ.html">Univariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/stat_univ_lab_brain-volume.html">Lab Brain volumes study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_multiv.html">Multivariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/time_series.html">Time Series in python</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">Linear dimension reduction and feature extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="manifold.html">Manifold learning: non-linear dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear methods for regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_classification.html">Linear classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="non_linear_prediction.html">Non linear learning algorithms</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Resampling Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#train-validation-and-test-sets">Train, Validation and Test Sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-validation-cv">Cross-Validation (CV)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallel-computation-with-joblib">Parallel computation with joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cv-for-model-selection-setting-the-hyper-parameters">CV for model selection: setting the hyper parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-permutations">Random Permutations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bootstrapping">Bootstrapping</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ensemble_learning.html">Ensemble learning: bagging, boosting and stacking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/optim_gradient_descent.html">Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_lab_face_recognition.html">Lab: Faces recognition using various learning models</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_backprop_numpy-pytorch-sklearn.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_mlp_mnist_pytorch.html">Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_cnn_cifar10_pytorch.html">Convolutional neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_transfer-learning_cifar10-ants-bees_pytorch.html">Transfer Learning Tutorial</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="non_linear_prediction.html" title="previous chapter">Non linear learning algorithms</a></li>
      <li>Next: <a href="ensemble_learning.html" title="next chapter">Ensemble learning: bagging, boosting and stacking</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/machine_learning/resampling.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>